[
  {
    "title": "Future-Ready: Building Tomorrow’s Tech Workforce",
    "authors": [],
    "organization": "CSET",
    "abstract": "Please join the Center for Security and Emerging Technology on Tuesday, March 10, for our 2026 Spring Symposium, which will include discussions on building the AI and emerging technology workforce. The event will feature a fireside chat with Senator Lisa Blunt Rochester, as well as insights from industry and labor experts at the cutting edge of technology workforce policy.",
    "url": "https://cset.georgetown.edu/news",
    "published_date": "2026-03-10T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://cset.georgetown.edu/publications/",
    "fetched_at": "2026-02-19T09:51:01.096229+00:00"
  },
  {
    "title": "Why we should expect ruthless sociopath ASI",
    "authors": [
      "Steven Byrnes"
    ],
    "organization": "Alignment Forum",
    "abstract": "The conversation begins (Fictional) Optimist: So you expect future artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies? Me: Yup! (Alas.) Optimist: …Despite all the evidence right in front of our eyes from humans and LLMs. Me: Yup! Optimist: OK, well, I’m here to tell you: that is a very specific and strange thing to expect, especially in the absence of any concrete evidence whatsoever. There’s no reason to expect it. If you think that ruthless sociopathy is the “true core nature of intelligence” or whatever, then&nbsp; you should really look at yourself in a mirror and ask yourself where your life went horribly wrong .",
    "url": "https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi",
    "published_date": "2026-02-18T17:28:17+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T09:50:51.894964+00:00"
  },
  {
    "title": "A new way to express yourself: Gemini can now create music",
    "authors": [
      "Google DeepMind News"
    ],
    "organization": "Google DeepMind",
    "abstract": "The Gemini app now features our most advanced music generation model Lyria 3, empowering anyone to make 30-second tracks using text or images.",
    "url": "https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music/",
    "published_date": "2026-02-18T16:01:38+00:00",
    "source_type": "rss",
    "source_url": "https://deepmind.google/blog/rss.xml",
    "fetched_at": "2026-02-19T09:50:47.806389+00:00"
  },
  {
    "title": "How We Protect Confidential Information",
    "authors": [],
    "organization": "METR",
    "abstract": "Our high-level approach to protecting confidential access and information Shared components of AI lab commitments to evaluate and mitigate severe risks. External review from METR of Anthropic's Summer 2025 Sabotage Risk Report Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI. How we think about tradeoffs when communicating surprising or nuanced findings. Current views on information relevant for visibility into frontier AI risk. Suggested priorities for the Office of Science and Technology Policy as it develops an AI Action Plan. Why legible and faithful reasoning is valuable for safely developing powerful AI List of frontier safety policies published by AI companies, including Amazon, Anthropic, Google DeepMind, G42, Meta, Microsoft, OpenAI, and xAI.",
    "url": "https://metr.org/blog/2026-02-17-how-we-protect-confidential-information/",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-19T09:50:53.943246+00:00"
  },
  {
    "title": "Boundary Point Jailbreaking: A new way to break the strongest AI defences",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Introducing an automated attack technique that generates universal jailbreaks against the best defended systems",
    "url": "https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-19T09:50:59.548192+00:00"
  },
  {
    "title": "Will reward-seekers respond to distant incentives?",
    "authors": [
      "Alex Mallen"
    ],
    "organization": "Alignment Forum",
    "abstract": "Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can't prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it's responding to incentives developers don’t control.",
    "url": "https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives",
    "published_date": "2026-02-16T19:35:12+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T09:50:51.895199+00:00"
  },
  {
    "title": "“Will reward-seekers respond to distant incentives?” by Alex Mallen",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren't?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.",
    "url": "https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant",
    "published_date": "2026-02-16T00:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-19T09:50:50.787049+00:00"
  },
  {
    "title": "AXRP Episode 48 - Guive Assadi on AI Property Rights",
    "authors": [
      "DanielFilan"
    ],
    "organization": "Alignment Forum",
    "abstract": "YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...",
    "url": "https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights",
    "published_date": "2026-02-15T02:20:55+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T09:50:51.896457+00:00"
  },
  {
    "title": "The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?",
    "authors": [],
    "organization": "Anthropic",
    "abstract": "Research done as part of the first Anthropic Fellows Program during Summer 2025.",
    "url": "https://alignment.anthropic.com/2026/hot-mess-of-ai/",
    "published_date": "2026-02-15T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://alignment.anthropic.com/",
    "fetched_at": "2026-02-19T09:50:53.392530+00:00"
  },
  {
    "title": "GPT-5.2 derives a new result in theoretical physics",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
    "url": "https://openai.com/index/new-result-theoretical-physics",
    "published_date": "2026-02-13T11:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-19T09:50:49.432133+00:00"
  },
  {
    "title": "What do “economic value” benchmarks tell us?",
    "authors": [],
    "organization": "Epoch AI",
    "abstract": "These benchmarks track a wide range of digital work. Progress will correlate with economic utility, but tasks are too self-contained to indicate full automation.",
    "url": "https://epoch.ai/blog/what-do-economic-value-benchmarks-tell-us",
    "published_date": "2026-02-13T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://epoch.ai/blog",
    "fetched_at": "2026-02-19T09:51:03.397873+00:00"
  },
  {
    "title": "models have some pretty funny attractor states",
    "authors": [
      "aryaj"
    ],
    "organization": "LessWrong",
    "abstract": "This work was conducted during the MATS 9.0 program under Neel Nanda and Senthooran Rajamanoharan. …",
    "url": "https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states",
    "published_date": "2026-02-12T21:14:52.004000+00:00",
    "source_type": "rss",
    "source_url": "https://www.lesswrong.com/graphql",
    "fetched_at": "2026-02-19T09:51:06.374456+00:00"
  },
  {
    "title": "Human-like metacognitive skills will reduce LLM slop and aid alignment and capabilities",
    "authors": [
      "Seth Herd"
    ],
    "organization": "Alignment Forum",
    "abstract": "1. Summary and overview LLMs seem to lack metacognitive skills that help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving capabilities in new directions. Better metacognition would reduce LLM errors by catching mistakes, and by managing complex cognition to produce better answers in the first place. This could stabilize or regularize alignment, allowing systems to avoid actions they would not \"endorse on reflection\" (in some functional sense). [1] Better metacognition could also make LLM systems useful for clarifying the conceptual problems of alignment. It would reduce sycophancy, and help LLMs organize the complex thinking necessary for clarifying claims and cruxes in the literature. Without such improvements, collaborating with LLM systems on alignment research could be the median doom-path: slop, not scheming . They are sycophantic, agreeing with their users too much, and produce compelling-but-erroneous \"slop\".",
    "url": "https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid",
    "published_date": "2026-02-12T19:38:50+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T09:50:51.896765+00:00"
  },
  {
    "title": "How do we (more) safely defer to AIs?",
    "authors": [
      "ryan_greenblatt"
    ],
    "organization": "Alignment Forum",
    "abstract": "As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible . [1] Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say \"deferring to AIs\" [2] I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions. [3] If we plan to defer to AIs, I think it's safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.",
    "url": "https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais",
    "published_date": "2026-02-12T16:55:52+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T09:50:51.898099+00:00"
  }
]