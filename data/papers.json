[
  {
    "title": "Future-Ready: Building Tomorrow’s Tech Workforce",
    "authors": [],
    "organization": "CSET",
    "abstract": "Please join the Center for Security and Emerging Technology on Tuesday, March 10, for our 2026 Spring Symposium, which will include discussions on building the AI and emerging technology workforce. The event will feature a fireside chat with Senator Lisa Blunt Rochester, as well as insights from industry and labor experts at the cutting edge of technology workforce policy.",
    "url": "https://cset.georgetown.edu/news",
    "published_date": "2026-03-10T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://cset.georgetown.edu/publications/",
    "fetched_at": "2026-02-21T09:32:08.882763+00:00"
  },
  {
    "title": "How will we do SFT on models with opaque reasoning?",
    "authors": [
      "Alek Westover"
    ],
    "organization": "Alignment Forum",
    "abstract": "Current LLMs externalize lots of their reasoning in human interpretable language. This reasoning is sometimes unfaithful , sometimes strange and concerning , and LLMs can do somewhat impressive reasoning without using CoT , but my overall impression is that CoT currently is a reasonably complete and accurate representation of LLM reasoning. However, reasoning in interpretable language might turn out to be uncompetitive—if so, it seems probable that opaque reasoning will be adopted in frontier AI labs. If future AI models have opaque reasoning, this will probably change what training we can apply to these AIs. For example, currently we train models to reason in a good way about math problems, or to reason in a desired way about the spec that we hope they’ll follow.",
    "url": "https://www.alignmentforum.org/posts/GJTzhQgaRWLFJkPbt/how-will-we-do-sft-on-models-with-opaque-reasoning",
    "published_date": "2026-02-21T00:00:17+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-21T09:32:03.953571+00:00"
  },
  {
    "title": "Our First Proof submissions",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "We share our AI model’s proof attempts for the First Proof math challenge, testing research-grade reasoning on expert-level problems.",
    "url": "https://openai.com/index/first-proof-submissions",
    "published_date": "2026-02-20T14:30:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-21T09:32:02.420549+00:00"
  },
  {
    "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
    "authors": [
      "Google DeepMind News"
    ],
    "organization": "Google DeepMind",
    "abstract": "3.1 Pro is designed for tasks where a simple answer isn’t enough.",
    "url": "https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks/",
    "published_date": "2026-02-19T16:06:14+00:00",
    "source_type": "rss",
    "source_url": "https://deepmind.google/blog/rss.xml",
    "fetched_at": "2026-02-21T09:32:01.091491+00:00"
  },
  {
    "title": "Funding 60 projects to advance AI alignment research",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The Alignment Project welcomes its first cohort of grantees, and new partners join the coalition, bringing total funding to £27m.",
    "url": "https://www.aisi.gov.uk/blog/funding-60-projects-to-advance-ai-alignment-research",
    "published_date": "2026-02-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-21T09:32:07.723527+00:00"
  },
  {
    "title": "Why we should expect ruthless sociopath ASI",
    "authors": [
      "Steven Byrnes"
    ],
    "organization": "Alignment Forum",
    "abstract": "The conversation begins (Fictional) Optimist: So you expect future artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies? Me: Yup! (Alas.) Optimist: …Despite all the evidence right in front of our eyes from humans and LLMs. Me: Yup! Optimist: OK, well, I’m here to tell you: that is a very specific and strange thing to expect, especially in the absence of any concrete evidence whatsoever. There’s no reason to expect it. If you think that ruthless sociopathy is the “true core nature of intelligence” or whatever, then&nbsp; you should really look at yourself in a mirror and ask yourself where your life went horribly wrong .",
    "url": "https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi",
    "published_date": "2026-02-18T17:28:17+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-21T09:32:03.953731+00:00"
  },
  {
    "title": "A new way to express yourself: Gemini can now create music",
    "authors": [
      "Google DeepMind News"
    ],
    "organization": "Google DeepMind",
    "abstract": "The Gemini app now features our most advanced music generation model Lyria 3, empowering anyone to make 30-second tracks using text or images.",
    "url": "https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music/",
    "published_date": "2026-02-18T16:01:38+00:00",
    "source_type": "rss",
    "source_url": "https://deepmind.google/blog/rss.xml",
    "fetched_at": "2026-02-21T09:32:01.091530+00:00"
  },
  {
    "title": "How We Protect Confidential Information",
    "authors": [],
    "organization": "METR",
    "abstract": "Our high-level approach to protecting confidential access and information Shared components of AI lab commitments to evaluate and mitigate severe risks. External review from METR of Anthropic's Summer 2025 Sabotage Risk Report Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI. How we think about tradeoffs when communicating surprising or nuanced findings. Current views on information relevant for visibility into frontier AI risk. Suggested priorities for the Office of Science and Technology Policy as it develops an AI Action Plan. Why legible and faithful reasoning is valuable for safely developing powerful AI List of frontier safety policies published by AI companies, including Amazon, Anthropic, Google DeepMind, G42, Meta, Microsoft, OpenAI, and xAI.",
    "url": "https://metr.org/blog/2026-02-17-how-we-protect-confidential-information/",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-21T09:32:04.991963+00:00"
  },
  {
    "title": "Boundary Point Jailbreaking: A new way to break the strongest AI defences",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Introducing an automated attack technique that generates universal jailbreaks against the best defended systems",
    "url": "https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-21T09:32:07.724394+00:00"
  },
  {
    "title": "Will reward-seekers respond to distant incentives?",
    "authors": [
      "Alex Mallen"
    ],
    "organization": "Alignment Forum",
    "abstract": "Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can't prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it's responding to incentives developers don’t control.",
    "url": "https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives",
    "published_date": "2026-02-16T19:35:12+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-21T09:32:03.953969+00:00"
  },
  {
    "title": "Don't Trust the Salt: AI Summarization, Multilingual Safety, and LLM Guardrails",
    "authors": [
      "benbreen"
    ],
    "organization": "Hacker News",
    "abstract": "Hacker News discussion with 221 points and 87 comments.",
    "url": "https://royapakzad.substack.com/p/multilingual-llm-evaluation-to-guardrails",
    "published_date": "2026-02-16T17:57:46+00:00",
    "source_type": "rss",
    "source_url": "https://news.ycombinator.com/",
    "fetched_at": "2026-02-21T09:32:13.365999+00:00"
  },
  {
    "title": "“Will reward-seekers respond to distant incentives?” by Alex Mallen",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren't?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.",
    "url": "https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant",
    "published_date": "2026-02-16T00:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-21T09:32:03.346810+00:00"
  },
  {
    "title": "AXRP Episode 48 - Guive Assadi on AI Property Rights",
    "authors": [
      "DanielFilan"
    ],
    "organization": "Alignment Forum",
    "abstract": "YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...",
    "url": "https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights",
    "published_date": "2026-02-15T02:20:55+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-21T09:32:03.954890+00:00"
  },
  {
    "title": "The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?",
    "authors": [],
    "organization": "Anthropic",
    "abstract": "Research done as part of the first Anthropic Fellows Program during Summer 2025.",
    "url": "https://alignment.anthropic.com/2026/hot-mess-of-ai/",
    "published_date": "2026-02-15T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://alignment.anthropic.com/",
    "fetched_at": "2026-02-21T09:32:04.850121+00:00"
  }
]