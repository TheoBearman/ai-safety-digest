[
  {
    "title": "A new way to express yourself: Gemini can now create music",
    "authors": [
      "Google DeepMind News"
    ],
    "organization": "Google DeepMind",
    "abstract": "The Gemini app now features our most advanced music generation model Lyria 3, empowering anyone to make 30-second tracks using text or images.",
    "url": "https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music/",
    "published_date": "2026-02-18T16:01:38+00:00",
    "source_type": "rss",
    "source_url": "https://deepmind.google/blog/rss.xml",
    "fetched_at": "2026-02-18T17:12:57.463723+00:00"
  },
  {
    "title": "How We Protect Confidential Information",
    "authors": [],
    "organization": "METR",
    "abstract": "Our high-level approach to protecting confidential access and information Shared components of AI lab commitments to evaluate and mitigate severe risks. External review from METR of Anthropic's Summer 2025 Sabotage Risk Report Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI. How we think about tradeoffs when communicating surprising or nuanced findings. Current views on information relevant for visibility into frontier AI risk. Suggested priorities for the Office of Science and Technology Policy as it develops an AI Action Plan. Why legible and faithful reasoning is valuable for safely developing powerful AI List of frontier safety policies published by AI companies, including Amazon, Anthropic, Google DeepMind, G42, Meta, Microsoft, OpenAI, and xAI.",
    "url": "https://metr.org/blog/2026-02-17-how-we-protect-confidential-information/",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-18T17:13:00.827928+00:00"
  },
  {
    "title": "Boundary Point Jailbreaking: A new way to break the strongest AI defences",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Introducing an automated attack technique that generates universal jailbreaks against the best defended systems",
    "url": "https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-18T17:13:03.439337+00:00"
  },
  {
    "title": "Will reward-seekers respond to distant incentives?",
    "authors": [
      "Alex Mallen"
    ],
    "organization": "Alignment Forum",
    "abstract": "Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can't prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it's responding to incentives developers don’t control.",
    "url": "https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives",
    "published_date": "2026-02-16T19:35:12+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-18T17:12:59.854467+00:00"
  },
  {
    "title": "“Will reward-seekers respond to distant incentives?” by Alex Mallen",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren't?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.",
    "url": "https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant",
    "published_date": "2026-02-16T00:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-18T17:12:59.433795+00:00"
  },
  {
    "title": "AXRP Episode 48 - Guive Assadi on AI Property Rights",
    "authors": [
      "DanielFilan"
    ],
    "organization": "Alignment Forum",
    "abstract": "YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...",
    "url": "https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights",
    "published_date": "2026-02-15T02:20:55+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-18T17:12:59.855746+00:00"
  },
  {
    "title": "The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?",
    "authors": [],
    "organization": "Anthropic",
    "abstract": "Research done as part of the first Anthropic Fellows Program during Summer 2025.",
    "url": "https://alignment.anthropic.com/2026/hot-mess-of-ai/",
    "published_date": "2026-02-15T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://alignment.anthropic.com/",
    "fetched_at": "2026-02-18T17:13:00.706014+00:00"
  },
  {
    "title": "GPT-5.2 derives a new result in theoretical physics",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
    "url": "https://openai.com/index/new-result-theoretical-physics",
    "published_date": "2026-02-13T11:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-18T17:12:58.780111+00:00"
  },
  {
    "title": "What do “economic value” benchmarks tell us?",
    "authors": [],
    "organization": "Epoch AI",
    "abstract": "These benchmarks track a wide range of digital work. Progress will correlate with economic utility, but tasks are too self-contained to indicate full automation.",
    "url": "https://epoch.ai/blog/what-do-economic-value-benchmarks-tell-us",
    "published_date": "2026-02-13T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://epoch.ai/blog",
    "fetched_at": "2026-02-18T17:13:05.538085+00:00"
  },
  {
    "title": "models have some pretty funny attractor states",
    "authors": [
      "aryaj"
    ],
    "organization": "LessWrong",
    "abstract": "This work was conducted during the MATS 9.0 program under Neel Nanda and Senthooran Rajamanoharan. …",
    "url": "https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states",
    "published_date": "2026-02-12T21:14:52.004000+00:00",
    "source_type": "rss",
    "source_url": "https://www.lesswrong.com/graphql",
    "fetched_at": "2026-02-18T17:13:07.622272+00:00"
  },
  {
    "title": "Human-like metacognitive skills will reduce LLM slop and aid alignment and capabilities",
    "authors": [
      "Seth Herd"
    ],
    "organization": "Alignment Forum",
    "abstract": "1. Summary and overview LLMs seem to lack metacognitive skills that help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving capabilities in new directions. Better metacognition would reduce LLM errors by catching mistakes, and by managing complex cognition to produce better answers in the first place. This could stabilize or regularize alignment, allowing systems to avoid actions they would not \"endorse on reflection\" (in some functional sense). [1] Better metacognition could also make LLM systems useful for clarifying the conceptual problems of alignment. It would reduce sycophancy, and help LLMs organize the complex thinking necessary for clarifying claims and cruxes in the literature. Without such improvements, collaborating with LLM systems on alignment research could be the median doom-path: slop, not scheming . They are sycophantic, agreeing with their users too much, and produce compelling-but-erroneous \"slop\".",
    "url": "https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid",
    "published_date": "2026-02-12T19:38:50+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-18T17:12:59.856089+00:00"
  },
  {
    "title": "How do we (more) safely defer to AIs?",
    "authors": [
      "ryan_greenblatt"
    ],
    "organization": "Alignment Forum",
    "abstract": "As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible . [1] Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say \"deferring to AIs\" [2] I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions. [3] If we plan to defer to AIs, I think it's safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.",
    "url": "https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais",
    "published_date": "2026-02-12T16:55:52+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-18T17:12:59.857441+00:00"
  },
  {
    "title": "Research note: A simpler AI timelines model predicts 99% AI R&D automation in ~2032",
    "authors": [
      "Thomas Kwa"
    ],
    "organization": "Alignment Forum",
    "abstract": "In this post, I describe a simple model for forecasting when AI will automate AI development. It is based on the AI Futures model , but more understandable and robust, and has deliberately conservative assumptions. At current rates of compute growth and algorithmic progress, this model's median prediction is &gt;99% automation of AI R&amp;D in late 2032. Most simulations result in a 1000x to 10,000,000x increase in AI efficiency and 300x-3000x research output by 2035. I therefore suspect that existing trends in compute growth and automation will still produce extremely powerful AI on \"medium\" timelines, even if the full coding automation and superhuman research taste that drive the AIFM's \"fast\" timelines (superintelligence by ~mid-2031) don't happen. Why make this? The AI Futures Model (AIFM) has 33 parameters; this has 8. I previously summarized the AIFM on LessWrong and found it to be very complex.",
    "url": "https://www.alignmentforum.org/posts/uy6B5rEPvcwi55cBK/research-note-a-simpler-ai-timelines-model-predicts-99-ai-r",
    "published_date": "2026-02-12T00:13:34+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-18T17:12:59.857716+00:00"
  },
  {
    "title": "“How do we (more) safely defer to AIs?” by Ryan Greenblatt",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: How can we make AIs aligned and well-elicited on extremely hard to check open ended tasks?. As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible.1 Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say “deferring to AIs”2 I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions.3 If we plan to defer to AIs, I think it's safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.4 For deference to go well, we both need it to be the case that the...",
    "url": "https://blog.redwoodresearch.org/p/how-do-we-more-safely-defer-to-ais",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-18T17:12:59.433853+00:00"
  },
  {
    "title": "International consensus and open questions in AI evaluations",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The International Network for Advanced AI Measurement, Evaluation and Science reflects on recent meeting and looks ahead to the India AI Impact Summit",
    "url": "https://www.aisi.gov.uk/blog/international-ai-network-consensus-and-open-questions",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-18T17:13:03.439796+00:00"
  },
  {
    "title": "What to Expect from AI in 2026: Q&A with William Marcellino",
    "authors": [],
    "organization": "RAND",
    "abstract": "Commentary Feb 12, 2026 RAND's William Marcellino directs the development of AI tools at RAND. He discusses how RAND is using AI, what AI might mean for human productivity, the limits of large language models, and more.",
    "url": "https://www.rand.org/pubs/commentary/2026/02/what-to-expect-from-ai-in-2026.html",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.rand.org/topics/artificial-intelligence.html",
    "fetched_at": "2026-02-18T17:13:03.769088+00:00"
  },
  {
    "title": "The Weapons of Mass Destruction AI Security Gap",
    "authors": [],
    "organization": "RAND",
    "abstract": "Commentary Feb 12, 2026 The AI ecosystem may be too narrowly focused on a single threat model: the “lone wolf virus terrorist.” This emphasis on individual actors could leave state-based and terrorist group threats dangerously under-examined.",
    "url": "https://www.rand.org/pubs/commentary/2026/02/the-weapons-of-mass-destruction-ai-security-gap.html",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.rand.org/topics/artificial-intelligence.html",
    "fetched_at": "2026-02-18T17:13:03.769305+00:00"
  },
  {
    "title": "Distinguish between inference scaling and \"larger tasks use more compute\"",
    "authors": [
      "ryan_greenblatt"
    ],
    "organization": "Alignment Forum",
    "abstract": "As many have observed, since reasoning models first came out , the amount of compute LLMs use to complete tasks has increased greatly. This trend is often called inference scaling and there is an open question of how much of recent AI progress is driven by inference scaling versus by other capability improvements . Whether inference compute is driving most recent AI progress matters because you can only scale up inference so far before costs are too high for AI to be useful (while training compute can be amortized over usage).",
    "url": "https://www.alignmentforum.org/posts/rRbDNQLfihiHbXytf/distinguish-between-inference-scaling-and-larger-tasks-use",
    "published_date": "2026-02-11T18:37:13+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-18T17:12:59.857807+00:00"
  }
]